year.diffs <- year.diffs %>%
select(-.) %>%
rename(diffs = year) %>%
group_by(keyword) %>%
mutate(position = 1:n())
# ungroup so later results have no problems
year.diffs <- year.diffs %>%
ungroup()
###
### conceptual structure networks
###
# create bigrams/edge list
akw.bigrams.noyears <- data.bioeth.1 %>%
filter(!is.na(author.keywords)) %>%
filter(!str_detect(author.keywords, "\\d")) %>%
group_by(year.pub) %>%
unnest_tokens(akw.bi,
author.keywords,
token = "ngrams",
n = 2) %>%
separate(akw.bi,
c("word1", "word2"),
sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(word1 != "bioethics"
& word1 != "ethics") %>%
filter(word2 != "bioethics"
& word2 != "ethics") %>%
ungroup()
# remove duplicates
akw.bigrams.noyears <- akw.bigrams.noyears %>%
filter(!duplicated(akw.bigrams.noyears))
# get edge weights
bigram.counts.noyears <- akw.bigrams.noyears %>%
count(word1, word2, sort = TRUE)
# another way to accomplish same task
# bigram.counts.noyears <- akw.bigrams.noyears %>%
# add_count(word1, word2)
# create undirected network from weighted bigram data
network <- bigram.counts.noyears %>%
rename(weight = n) %>%
graph_from_data_frame(directed = FALSE)
# create focal attribute data for keywords in the network
# create empty list in which to store results
#focal.attribs <- vector("list", length = nrow(unique.akw.final))
#  # length is number of variables about keyword
#for (i in 1:length(focal.attribs)) {
#  focal.attribs[[i]] <- bigram.counts.noyears %>%
#    filter(word1 == unique.akw.final$akw[i]) %>%
#    summarise(word = unique.akw.final$akw[i],
#              sd=sd(n),
#              min=min(n),
#              max=max(n),
#              mean=mean(n),
#              range=max(n)-min(n),
#              iqr = stats::IQR(n),
#              degree=n())
#}
# create data frame from list
#focal.attribs.tbl <- bind_rows(focal.attribs)
# get degree distribution
#unique.deg.network <- focal.attribs.tbl %>%
#  count(degree, sort = TRUE)
# supposedly fixes omission of words that appear in word2 but not word1
#focal.attribs1 <- vector("list", length = nrow(unique.akw.final))
# length is number of variables about keyword
#for (i in 1:length(focal.attribs1)) {
#  if (unique.akw.final$akw[i] %in% bigram.counts.noyears$word1){
#    focal.attribs1[[i]] <- bigram.counts.noyears %>%
#      filter(word1 == unique.akw.final$akw[i]) %>%
#      summarise(word = unique.akw.final$akw[i],
#                sd=sd(n),
#                min=min(n),
#                max=max(n),
#                mean=mean(n),
#                range=max(n)-min(n),
#                iqr = stats::IQR(n),
#                degree=n())
#  } else if (!unique.akw.final$akw[i] %in% bigram.counts.noyears$word1 &
#             unique.akw.final$akw[i] %in% bigram.counts.noyears$word2) {
#    focal.attribs1[[i]] <- bigram.counts.noyears %>%
#      filter(word2 == unique.akw.final$akw[i]) %>%
#      summarise(word = unique.akw.final$akw[i],
#                sd=sd(n),
#                min=min(n),
#                max=max(n),
#                mean=mean(n),
#                range=max(n)-min(n),
#                iqr = stats::IQR(n),
#                degree=n())
#  }
#}
#focal.attribs1 <- bind_rows(focal.attribs1)
# community structure
# edge betweenness DO NOT RUN!
# net.comm <- cluster_edge_betweenness(network, directed = FALSE)
# louvain algorithm
net.comm.louvain <- cluster_louvain(network)
# label propagation
comm.label.prop <- cluster_label_prop(network)
# infomap
comm.info.map <- cluster_infomap(network)
# walktrap
comm.walk.trap <- cluster_walktrap(network)
# temporal networks
# remove rows with numbers and duplicates
akw.art.final <- data.bioeth.1 %>%
filter(!is.na(author.keywords)) %>%
unnest_tokens(akw, author.keywords) %>%
anti_join(stop_words,
by = c("akw" = "word")) %>%
filter(akw != "bioethics" &
akw != "ethics") %>%
filter(!str_detect(akw, "\\d"))
# create bigram data with years (for edgelist)
akw.bigrams.years <- data.bioeth.1 %>%
filter(!is.na(author.keywords)) %>%
filter(!str_detect(author.keywords, "\\d")) %>%
group_by(year.pub) %>%
unnest_tokens(akw.bi,
author.keywords,
token = "ngrams",
n = 2) %>%
separate(akw.bi,
c("word1", "word2"),
sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(word1 != "bioethics"
& word1 != "ethics") %>%
filter(word2 != "bioethics"
& word2 != "ethics") %>%
ungroup()
# remove duplicates
akw.bigrams.years <- akw.bigrams.years %>%
filter(!duplicated(akw.bigrams.years))
# get edge weights
bigram.counts.years <- akw.bigrams.years %>%
count(word1, word2, year.pub, sort = TRUE)
# move weight to third column for igraph
bigram.counts.years <- bigram.counts.years[,c("word1",
"word2",
"n",
"year.pub")]
# check for duplicates in non-yearly version
duplicates <- bigram.counts.noyears %>%
unite(bigram, word1, word2, sep = " ")
dup.vec <- duplicates %>%
duplicated() # none true
# create decade variable for longitudinal bigrams
# CHANGE TO case_when()
bigram.counts.decades <- bigram.counts.years %>%
mutate(decade = ifelse(year.pub %in% 1970:1979, "1970s",
ifelse(year.pub %in% 1980:1989,"1980s",
ifelse(year.pub %in% 1990:1999, "1990s",
ifelse(year.pub %in% 2000:2009, "2000s",
ifelse(year.pub %in% 2010:2018, "2010s", NA))))))
bigram.counts.decades$decade <- factor(bigram.counts.decades$decade,
levels = c("1970s", "1980s",
"1990s", "2000s",
"2010s"))
# create table with descriptives for each time period
# years was already created (vector 1990-2010)
network.years <- vector("list", length = length(years))
stats.years <- vector("list", length = length(years))
for (i in 1:length(network.years)) {
# create network
network.years[[i]] <- bigram.counts.years %>%
filter(year.pub == years[i]) %>%
rename(weight = n) %>%
select(-year.pub) %>%
graph_from_data_frame(directed = FALSE)
network.years[[i]] <- simplify(network.years[[i]],
remove.multiple = FALSE,
remove.loops = TRUE)
# create descriptive stats object
stats.years[[i]] <- bigram.counts.years %>%
filter(year.pub == years[i]) %>%
summarise(num.nodes = vcount(network.years[[i]]),
num.edges = ecount(network.years[[i]]),
density = edge_density(network.years[[i]],
loops = FALSE),
year = years[i])
}
# do the same for decades
decades <- c("1990s", "2000s", "2010s")
network.decades <- vector("list",
length = length(decades))
stats.decades <- vector("list", length = length(decades))
for (i in 1:length(decades)) {
# create network
network.decades[[i]] <- bigram.counts.decades %>%
filter(decade == decades[i]) %>%
rename(weight = n) %>%
select(-year.pub) %>%
graph_from_data_frame(directed = FALSE)
network.years[[i]] <- simplify(network.decades[[i]],
remove.multiple = FALSE,
remove.loops = TRUE)
# create descriptive stats object
stats.decades[[i]] <- bigram.counts.decades %>%
filter(decade == decades[i]) %>%
summarise(num.nodes = vcount(network.decades[[i]]),
num.edges = ecount(network.decades[[i]]),
density = edge_density(network.decades[[i]],
loops = FALSE),
decade = decades[i])
}
# combine longitudinal descriptives
stats.combined <- bind_rows(stats.years, stats.decades)
stats.graph <- stats.combined %>%
filter(!is.na(year), year < 2018) %>%
select(num.edges, num.nodes, year)
stats.melt <- melt(data = stats.graph, id.vars = "year",
measure.vars = c("num.edges","num.nodes"),
na.rm = TRUE)
# calculate above stats for overall (1990-2018) network
# remove self loops
network <- simplify(network, remove.multiple = FALSE,
remove.loops = TRUE)
# create descriptive stats object
stats.collapsed <- bigram.counts.decades %>%
filter(decade == decades[i]) %>%
summarise(num.nodes = vcount(network),
num.edges = ecount(network),
density = edge_density(network,
loops = FALSE),
decade = "1990-2018")
# combine longitudinal and cross-sectional summary tables
stats.combined <- bind_rows(stats.combined,
stats.collapsed)
# what is the lifespan of a tie?
bigram.life <- bigram.counts.years %>%
unite(bigram, word1, word2, sep = " ") %>%
group_by(bigram) %>%
summarise(lifespan = max(year.pub) - min(year.pub),
mean.weight = mean(n),
max.weight = max(n),
min.weight = min(n),
sd.weight = sd(n))
for (i in 1:nrow(bigram.life)) {
if (bigram.life$lifespan[i] == 0) {
bigram.life$lifespan[i] <- 1
bigram.life$sd.weight[i] <- NA
}
}
# create bipartite cross-sectional network
akw.art <- akw.art.final %>%
select(akw, article.id) %>%
as.data.frame()
igraph.noyears <- graph_from_data_frame(akw.art, directed = FALSE)
# set modes
V(igraph.noyears)$type <- V(igraph.noyears)$name %in% akw.art[,2]
# the second column of edges is TRUE type
# create tidy version of above graph
tidygraph.noyears <- as_tbl_graph(igraph.noyears)
###
### bieothics relational analyses
###
# getting conceptual profile data
# get unique words and their counts
data("stop_words")
unique.akw <- data.bioeth.1 %>%
filter(!is.na(author.keywords)) %>%
unnest_tokens(akw, author.keywords) %>%
anti_join(stop_words, by = c("akw" = "word")) %>%
filter(akw != "bioethics" & akw != "ethics") %>%
count(akw, sort = TRUE)
# which cases contain numbers
unique.akw.nonum <- grepl("\\d", unique.akw$akw)
# remove cases with numbers
unique.akw.final <- unique.akw %>%
filter(unique.akw.nonum==FALSE)
# get focal attributes about keywords
# create empty vector
focal.attribs1 <- vector("list", length = nrow(unique.akw.final))
# length is number of variables about keyword
# populate list with focal attribute data
for (i in 1:length(focal.attribs1)) {
if (unique.akw.final$akw[i] %in% bigram.counts.noyears$word1){
focal.attribs1[[i]] <- bigram.counts.noyears %>%
filter(word1 == unique.akw.final$akw[i]) %>%
summarise(word = unique.akw.final$akw[i],
sd=sd(n),
min=min(n),
max=max(n),
mean=mean(n),
range=max(n)-min(n),
iqr = stats::IQR(n),
degree=n())
} else if (!unique.akw.final$akw[i] %in% bigram.counts.noyears$word1 &
unique.akw.final$akw[i] %in% bigram.counts.noyears$word2) {
focal.attribs1[[i]] <- bigram.counts.noyears %>%
filter(word2 == unique.akw.final$akw[i]) %>%
summarise(word = unique.akw.final$akw[i],
sd=sd(n),
min=min(n),
max=max(n),
mean=mean(n),
range=max(n)-min(n),
iqr = stats::IQR(n),
degree=n())
}
}
# collapse list into single object
focal.attribs1 <- bind_rows(focal.attribs1)
# previous problem solving
#weird.cases <- focal.attribs.tbl %>%
#  filter(degree == 0)
#weird.cases$problem <- NA
#for (i in 1:nrow(weird.cases)) {
#  if (weird.cases$word[i] %in% bigram.counts.noyears$word1 |
#      weird.cases$word[i] %in% bigram.counts.noyears$word2) {
#    weird.cases$problem[i] <- TRUE
#  } else {
#    weird.cases$problem[i] <- FALSE
#  }
#}
# get clustering coeffient for each node/keyword in
# keyword-keyword network
# collapse bipartite network
keyword.projection <- bipartite.projection(tidygraph.noyears,
which = "false")
# get transitivity/clustering
keyword.projection.lcc <- transitivity(keyword.projection,
type = c("barrat"))
# make tibble and add keyword name as a variable
keyword.projection.lcc <- as_tibble(keyword.projection.lcc) %>%
mutate(keyword = V(keyword.projection)$name)
# create final keyword attribute data set
keyword.data.0 <- left_join(focal.attribs1,
keyword.projection.lcc,
by = c("word" = "keyword"))
# rename lcc
keyword.data.0 <- keyword.data.0 %>%
rename(lcc = value)
# recode consec to intuitive names
keyword.evo$consec[keyword.evo$consec==TRUE] <- "persist"
keyword.evo$consec[keyword.evo$consec==FALSE] <- "recur"
keyword.evo$consec[is.na(keyword.evo$consec)] <- "single"
# add keyword.evo data to keyword.data.0
keyword.data.0 <- left_join(keyword.data.0, keyword.evo,
by = c("word" = "keyword"))
#####
### get subset of paper_i's keywords
# create empty list for sets of node ids
#paper.keywords <- vector(mode = "list", length =
#                          length(V(igraph.noyears)[type==FALSE]))
# create function to get a paper's keywords
get.keywords <- function(paper.id) { # paper.id must be numeric
#data.bioeth.1 %>%
#  filter(article.id == paper.id) %>%
#  select(author.keywords)
akw.art.final$akw[akw.art.final$article.id == paper.id] %>%
as_tibble() %>%
rename(akw = value)
}
# create list for which each element is a paper's keyword(s)
paper.ego <- lapply(data.bioeth.1$article.id, get.keywords)
# this works but does not provide article identifiers
# could alter function such that each list element is
# a list containing two elements: article id and keywords
# get keyword ego networks
# WHY DO I NEED THIS?
# keyword.ego <- ego(graph = keyword.projection,
#                    order = 1,
#                    nodes = V(keyword.projection),
#                    mode = "all")
# if weight > 1
#tidy.proj <- as_tbl_graph(keyword.projection)
#tidy.proj %>%
#  activate(edges) %>%
#  filter(from == 1) %>%
#  as_tibble() %>%
#  summarise(num.alters = n(),
#            max.weight = max(weight),
#            range.weight = max.weight - min(weight),
#            sd.weight = sd(weight))
#####
### save vector of node/keyword id names aka dictionary/look up table
# create function to get a word's id
get.id <- function(x){
which(V(keyword.projection)$name == x)
}
# apply function to all unique words
node.key <- sapply(unique.akw.final$akw, get.id)
# make look up table/key/dictionary
node.key <- node.key %>%
as_tibble() %>%
tibble::rownames_to_column(var = "keyword") %>%
rename(node.id = value)
#####
### get subset of bipartite network based upon names
# need to add num.id to paper.ego tibble elements
# add num.id
paper.ego <- lapply(paper.ego, function(i) {
inner_join(i, node.key, by = c("akw" = "keyword" ))
})
# create empty list in which to store results
#ego.filtered <- vector(mode = "list", length = paper.ego)
# filter keyword.projection based upon paper.ego results/list elements
ego.nets <- lapply(paper.ego, function(i){
induced_subgraph(keyword.projection, i$node.id)
})
# save edge weights in new object (MAY BE UNNECESSARY)
# save standarized weights in a new object?
# on what to standarize: whole network, ego network?
#####
### calculate descriptive stats of weight distribution
# write function to calculate descriptives for an ego network
# provide a graph (x)
describe.ego.net <- function(x){
if (length(E(x)$weight) == 0) {
as_tibble(E(x)$weight) %>%
summarise(degree = n(),
mean.weight = NA, # number of edges, remove
median.weight = NA,
min.weight = NA,
max.weight = NA,
range.weight = NA,
sd.weight = NA)
} else {
as_tibble(E(x)$weight) %>%
summarise(degree = n(), # number of edges, remove
mean.weight = mean(value),
median.weight = median(value),
min.weight = min(value),
max.weight = max(value),
range.weight = max.weight - min.weight,
sd.weight = sd(value))
}
} # could also mark missing after summary creation with separate function
# get descriptives for all ego networks
paper.ego.weight.desc <- lapply(ego.nets, describe.ego.net)
# might add paper id to output elements
# make a tibble to merge with paper-as-unit data
paper.ego.weight.desc <- bind_rows(paper.ego.weight.desc)
### merge with paper-as-unit data
data.bioeth.1 <- bind_cols(data.bioeth.1, paper.ego.weight.desc)
# remove the now unnecessary object
rm(paper.ego.weight.desc)
#####
### add keyword type variable (i.e., persist, recur, single)
# number of persist
# number of recur
# number of single
# total number of keywords
# description about number of years active (NEED TO DO THIS)
get.keyword.types <- function(ego.network){
if(class(ego.network) == "igraph") {
if(length(V(ego.network)) > 0){
as_tibble(V(ego.network)$name) %>%
left_join(keyword.evo, by = c("value" = "keyword")) %>%
summarise(num.single = sum(consec == "single"),
num.recur = sum(consec == "recur"),
num.persist = sum(consec == "persist"),
num.keywords = n(),
prop.single = num.single / num.keywords,
prop.recur = num.recur / num.keywords,
prop.persist = num.persist / num.keywords)
} else {
as_tibble(V(ego.network)$name) %>%
left_join(keyword.evo, by = c("value" = "keyword")) %>%
summarise(num.single = NA,
num.recur = NA,
num.persist = NA,
num.keywords = NA,
prop.single = NA,
prop.recur = NA,
prop.persist = NA)
}
} else {
stop("Input object not of igraph class.")
}
}
# get information on papers' keyword type usage
paper.keyword.type.desc <- lapply(ego.nets, get.keyword.types)
# collapse from list into data frame
paper.keyword.type.desc <- bind_rows(paper.keyword.type.desc)
# add to main data
data.bioeth.1 <- bind_cols(data.bioeth.1, paper.keyword.type.desc)
# remove now unnecessary object
rm(paper.keyword.type.desc)
####
### create the non-keyword-related variables
###
### paper level
# number of authors
data.bioeth.1 <- data.bioeth.1 %>%
mutate(num.authors = str_count(authors, ";") + 1)
# number of institutions
data.bioeth.1 <- data.bioeth.1 %>%
mutate(num.addresses = str_count(author.address, "\\["))
# a [] pair introduces each unique address
# number of cited references
data.bioeth.1 <- data.bioeth.1 %>%
mutate(num.refs = str_count(cited.refs, ";") + 1)
names(data.bioeth.1)
names(data.bioeth)
journal_names <- unique(data.bioeth$pub.name)
dim(journal_names)
length(journal_names)
head(journal_names, 20)
journal_names_counted <- count(data.bioeth$pub.name)
library(dplyr)
library(tidytext)
journal_names_counted <- data.bioeth %>% count(pub.name, sort = TRUE)
head(journal_names_counted)
getwd()
setwd("/Users/brandonsepulvado/Documents/bioethics-networks")
write.csv(journal_names_counted, file = "journal_names_counted.csv")
